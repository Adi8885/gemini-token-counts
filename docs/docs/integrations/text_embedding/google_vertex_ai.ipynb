{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Vertex AI Embeddings \n",
    "\n",
    ">[Vertex AI Embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) is a service on Google Cloud exposing the embedding models. \n",
    "\n",
    "Note: This integration is separate from the Google PaLM integration.\n",
    "\n",
    "By default, Google Cloud [does not use](https://cloud.google.com/vertex-ai/docs/generative-ai/data-governance#foundation_model_development) Customer Data to train its foundation models as part of Google Cloud`s AI/ML Privacy Commitment. More details about how Google processes data can also be found in [Google's Customer Data Processing Addendum (CDPA)](https://cloud.google.com/terms/data-processing-addendum).\n",
    "\n",
    "To use Vertex AI PaLM you must have the `langchain-google-vertexai` Python package installed and either:\n",
    "- Have credentials configured for your environment (gcloud, workload identity, etc...)\n",
    "- Store the path to a service account JSON file as the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "\n",
    "This codebase uses the `google.auth` library which first looks for the application credentials variable mentioned above, and then looks for system-level auth.\n",
    "\n",
    "For more information, see: \n",
    "- https://cloud.google.com/docs/authentication/application-default-credentials#GAC\n",
    "- https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "> langchain-google-vertexai : Install from PyPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings\n",
    ">Check the list of [Supported Models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models)\n",
    "\n",
    "Vertex AI text embeddings API uses dense vector representations: text-embedding-gecko, for example, uses 768-dimensional vectors. Dense vector embedding models use deep-learning methods similar to the ones used by large language models. Unlike sparse vectors, which tend to directly map words to numbers, dense vectors are designed to better represent the meaning of a piece of text. The benefit of using dense vector embeddings in generative AI is that instead of searching for direct word or syntax matches, you can better search for passages that align to the meaning of the query, even if the passages don't use the same language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the a specific Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"This is a test document.\"\n",
    "query = \"This is a test query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for a query\n",
    "query_emb = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for document(s)\n",
    "doc_emb = embeddings.embed_documents([document])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Embeddings Task Types](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types#supported_task_types)\n",
    "> Check the list of [Supported Models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types#supported_models) and [Supported task types](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types#supported_task_types)\n",
    "\n",
    "Vertex AI embeddings models can generate optimized embeddings for various task types, such as document retrieval, question and answering, and fact verification. Task types are labels that optimize the embeddings that the model generates based on your intended use case.\n",
    "\n",
    "<br>**Benefits of task types** : [Read More](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types#benefits_of_task_types)<br/>\n",
    "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/embeddings/images/task_type_1.png\" alt=\"isolated\" width=\"500\"/>\n",
    "<img src=\"https://cloud.google.com/static/vertex-ai/generative-ai/docs/embeddings/images/task_type_2.png\" alt=\"isolated\" width=\"500\"/>\n",
    "\n",
    "\n",
    "<p>Embeddings models that use task types support the following task types:</p>\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Task type</th>\n",
    "<th>Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><a href=\"#assess_text_similarity\"><code translate=\"no\" dir=\"ltr\">SEMANTIC_SIMILARITY</code></a></td>\n",
    "<td>Used to generate embeddings that are optimized to assess text similarity</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"#classify_texts\"><code translate=\"no\" dir=\"ltr\">CLASSIFICATION</code></a></td>\n",
    "<td>Used to generate embeddings that are optimized to classify texts according to preset labels</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"#cluster_texts\"><code translate=\"no\" dir=\"ltr\">CLUSTERING</code></a></td>\n",
    "<td>Used to generate embeddings that are optimized to cluster texts based on their similarities</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"#retrieve_information_from_texts\"><code translate=\"no\" dir=\"ltr\">RETRIEVAL_DOCUMENT</code>, <code translate=\"no\" dir=\"ltr\">RETRIEVAL_QUERY</code>,  <code translate=\"no\" dir=\"ltr\">QUESTION_ANSWERING</code>, and <code translate=\"no\" dir=\"ltr\">FACT_VERIFICATION</code></a></td>\n",
    "<td>Used to generate embeddings that are optimized for document search or information retrieval</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "<p>The best task type for your embeddings job depends on what use case you have for\n",
    "your embeddings. Before you select a task type, determine your embeddings use\n",
    "case.</p>\n",
    "\n",
    "NOTE : \n",
    "- To use specific task types, you need to use `embed` method\n",
    "- `embed_query` method uses task type `RETRIEVAL_QUERY` and `embed_documents` method uses `RETRIEVAL_DOCUMENT` implicitly to work seemlessly with Langchain RAG APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# Using Task Type with embed method\n",
    "text1 = \"The cat is sleeping\"\n",
    "text2 = \"The feline is napping\"\n",
    "\n",
    "# Preapre inputs for the embed method\n",
    "texts = [text1, text2]\n",
    "\n",
    "# get embeddings by passing task type as 'SEMANTIC_SIMILARITY'\n",
    "emb = embeddings.embed(texts=texts, embeddings_task_type=\"SEMANTIC_SIMILARITY\")\n",
    "\n",
    "print(len(emb[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Embeddings\n",
    "[Vertex Multilingual Embedding Models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models) can help you get best performance while working on use cases with languages other than English.\n",
    "\n",
    ">Check the list of [Supported Models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models) and [Supported Languages](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api#supported_text_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the a specific Multilingual Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-multilingual-embedding-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Task Type with embed method\n",
    "text1 = \"The cat is sleeping\"\n",
    "text2 = \"बिल्ली सो रही है\"  # Hindi translation for \"The cat is sleeping\"\n",
    "\n",
    "# Preapre inputs for the embed method\n",
    "texts = [text1, text2]\n",
    "\n",
    "# get embeddings by passing task type as 'SEMANTIC_SIMILARITY'\n",
    "emb = embeddings.embed(texts=texts, embeddings_task_type=\"SEMANTIC_SIMILARITY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get [text embeddings predictions in batches](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings)\n",
    ">Check the list of [Supported Models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings#text_embeddings_models_that_support_batch_predictions)\n",
    "\n",
    "Getting responses in a batch is a way to efficiently send large numbers of non-latency sensitive embeddings requests. Different from getting online responses, where you are limited to one input request at a time, you can send a large number of LLM requests in a single batch request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the a specific Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length input documents : 8\n",
      "length embedded documents : 8\n"
     ]
    }
   ],
   "source": [
    "# Prepare inputs for the embed method\n",
    "documents = [\"foo bar\"] * 8\n",
    "\n",
    "# get batch embeddings in single request\n",
    "batch_emb = embeddings.embed_documents(documents)\n",
    "\n",
    "# check lengthnumber of documents and retruned embeddings\n",
    "print(f\"length input documents : {len(documents)}\")\n",
    "print(f\"length embedded documents : {len(batch_emb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MultiModal Embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) \n",
    ">Check the list of [Supported Models](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#supported-models) and [Usage Limits](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)\n",
    "\n",
    "The multimodal embeddings model generates 1408-dimension vectors* based on the input you provide, which can include a combination of image, text, and video data. The embedding vectors can then be used for subsequent tasks like image classification or video content moderation. \n",
    "\n",
    "The image embedding vector and text embedding vector are in the same semantic space with the same dimensionality. Consequently, these vectors can be used interchangeably for use cases like searching image by text, or searching video by image.\n",
    "\n",
    "By default an embedding request returns a 1408 float vector for a data type. You can also specify lower-dimension embeddings (128, 256, or 512 float vectors) for text and image data. This option lets you optimize for latency and storage or quality based on how you plan to use the embeddings. Lower-dimension embeddings provide decreased storage needs and lower latency for subsequent embedding tasks (like search or recommendation), while higher-dimension embeddings offer greater accuracy for the same tasks.\n",
    "\n",
    "NOTE : For text-only embedding use cases, we recommend using the [Vertex AI text-embeddings API](#text-embeddings) instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the a specific Multimodel Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"multimodalembedding@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using embed_image method\n",
    "image_path = \"https://cloud.google.com/static/vertex-ai/generative-ai/docs/image/images/img-embedding-cat.jpg\"\n",
    "\n",
    "# get embeddings by passing task type as 'SEMANTIC_SIMILARITY'\n",
    "image_emb = embeddings.embed_image(image_path=image_path)\n",
    "\n",
    "# view embedding length\n",
    "len(image_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL : You may also provide `contextual_text` along with input image as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using embed_image method\n",
    "image_path = \"https://cloud.google.com/static/vertex-ai/generative-ai/docs/image/images/img-embedding-cat.jpg\"\n",
    "contextual_text = \"picture of a cat\"\n",
    "\n",
    "# get embeddings by passing task type as 'SEMANTIC_SIMILARITY'\n",
    "image_emb = embeddings.embed_image(\n",
    "    image_path=image_path, contextual_text=contextual_text\n",
    ")\n",
    "\n",
    "# view embedding length\n",
    "len(image_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Specify lower-dimension embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#low-dimension)\n",
    "By default an embedding request returns a 768 float vector for text embeddings and 1408 float vector for multimodal model. You can also specify lower-dimension embeddings (128, 256, or 512 float vectors) for text and image data. This option lets you optimize for latency and storage or quality based on how you plan to use the embeddings. Lower-dimension embeddings provide decreased storage needs and lower latency for subsequent embedding tasks (like search or recommendation), while higher-dimension embeddings offer greater accuracy for the same tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the a specific Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "\n",
    "# Embedding with reduced dimensionlaity\n",
    "dimension = 256\n",
    "\n",
    "# Using Task Type with embed method\n",
    "text1 = \"The cat is sleeping\"\n",
    "text2 = \"बिल्ली सो रही है\"  # Hindi translation for \"The cat is sleeping\"\n",
    "\n",
    "# Preapre inputs for the embed method\n",
    "texts = [text1, text2]\n",
    "\n",
    "# get embeddings by passing task type as 'SEMANTIC_SIMILARITY'\n",
    "emb = embeddings.embed(\n",
    "    texts=texts, embeddings_task_type=\"SEMANTIC_SIMILARITY\", dimensions=dimension\n",
    ")\n",
    "\n",
    "# Below code should output embedding with 256 dimensions\n",
    "len(emb[0])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m104"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc99336516f23363341912c6723b01ace86f02e26b4290be1efc0677e2e2ec24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
